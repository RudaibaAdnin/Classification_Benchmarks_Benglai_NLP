{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    " \n",
    "# The GPU id to use, usually either \"0\" or \"1\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries \n",
    "import sklearn\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline \n",
    "\n",
    "import string\n",
    "from os import listdir\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from pickle import dump\n",
    "from string import punctuation\n",
    "\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from pickle import load\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import csv, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Geopoitical Hate': 1, 'Political Hate': 4, 'Political Normal': 3, 'Personal Hate': 5, 'Religious Hate': 2, 'Gender abusive hate': 0}\n"
     ]
    }
   ],
   "source": [
    "categories = ['Gender abusive hate', 'Geopoitical Hate','Religious Hate','Political Normal','Political Hate','Personal Hate']\n",
    "num_of_labels=len(categories)\n",
    "categories_dict={key:value for value,key in enumerate(categories)}\n",
    "categories_inverse_dict={key:value for key,value in enumerate(categories)}\n",
    "print(categories_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['অতএব', 'অথচ']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "stop_words='stopwords_bn.txt'\n",
    "text_data=[]\n",
    "with open(stop_words,'r',encoding='utf-8') as temp_output_file:\n",
    "    reader=csv.reader(temp_output_file, delimiter='\\n')\n",
    "    for row in reader:\n",
    "        text_data.append(row)\n",
    "stop_word_list=[x[0] for x in text_data]\n",
    "print(stop_word_list[0:2])\n",
    "print(type(stop_word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(dataset, filename):\n",
    "    dump(dataset, open(filename,'wb'))\n",
    "    print('Saved :%s' % filename)\n",
    "\n",
    "def load_doc(filename):\n",
    "    file=open(filename,'r',encoding='utf-8')\n",
    "    text=file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def clean_doc(doc, stop_word_list):\n",
    "    # split into tokens by white space\n",
    "    sentences=list()\n",
    "    \n",
    "    for sentence in doc:\n",
    "        # remove punctuation from each token\n",
    "        \n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        tokens = [w.translate(table) for w in sentence.split(' ')]  # I belive word in sentence\n",
    "        sentences.append(' '.join(tokens))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_labels(filename,stop_word_list):\n",
    "    doc=load_doc(filename)\n",
    "    x_doc=list()\n",
    "    label=list()\n",
    "    for line in doc.split('\\n'):\n",
    "        x_and_label=line.split('\\t')\n",
    "        #print(x_and_label)\n",
    "        x_doc.append(x_and_label[1])\n",
    "        label.append(categories_dict[x_and_label[0]])\n",
    "        \n",
    "    #print(x_doc)\n",
    "    #print(label)\n",
    "    trainX = clean_doc(x_doc, stopwords)\n",
    "    return trainX, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_data_and_labels('Hate_Speech_Train.csv',stop_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 137  238 1016 ...    0    0    0]\n",
      " [   2  239  138 ...    0    0    0]\n",
      " [  35  139  140 ...    0    0    0]\n",
      " ...\n",
      " [   6   47    2 ...    0    0    0]\n",
      " [  36    8  580 ...    0    0    0]\n",
      " [ 298   98  138 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "    tokenizer=Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "def encode_text(tokenizer, lines, length):\n",
    "    #print(lines)\n",
    "    encoded=tokenizer.texts_to_sequences(lines)\n",
    "    print(encoded[0:10])\n",
    "    padded= pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenizer=create_tokenizer(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 300\n",
      "Vocabulary size: 3417\n",
      "\n",
      "[[137, 238, 1016, 576, 1017, 577, 65], [2, 239, 138, 1018, 32, 46, 32, 1019, 240, 578], [35, 139, 140, 1020, 1, 1021, 4, 1], [13, 81, 1022, 1023, 33, 1024, 14, 4, 1025], [295, 1026, 6, 1027, 162], [141, 114, 1028, 1029, 1030, 1031, 6, 1032, 1033, 579, 1034, 1035], [1036, 296, 1037, 1038, 1039, 1, 1040, 1041, 241, 97], [6, 47, 2, 1042, 1043, 163, 1044, 1045, 389, 297, 241, 1046, 1047, 1048, 24], [36, 8, 580, 1049, 1050, 1051, 1052], [298, 98, 138, 47, 191, 1053, 14, 1054, 1055]]\n",
      "(877, 300)\n"
     ]
    }
   ],
   "source": [
    "trainLength=300\n",
    "vocab_size=len(train_tokenizer.word_index)+1\n",
    "print('Max document length: %d' % trainLength)\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "print()\n",
    "X_train = encode_text(train_tokenizer, X_train, trainLength)\n",
    "print(X_train.shape)\n",
    "#trainX[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = Word2Vec.load('C:/Users/admin-karim/Desktop/BengWord2Vec/posts.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\admin-karim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "NUM_WORDS=20000\n",
    "EMBEDDING_DIM=300\n",
    "\n",
    "vocabulary_size=len(train_tokenizer.word_index)+1\n",
    "word_index=tokenizer.word_index\n",
    "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i>=NUM_WORDS:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector=word_vectors[word]\n",
    "        embedding_matrix[i]=embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
    "\n",
    "del(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer=Embedding(vocabulary_size, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model(length, vocab_size):\n",
    "\t# channel 1\n",
    "\tinput1 = Input(shape=(length,))\n",
    "\tembedding_layer_1 = embedding_layer(input1)\n",
    "\tconv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding_layer_1)\n",
    "\tdrop1 = Dropout(0.5)(conv1)\n",
    "\tpool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "\tflat1 = Flatten()(pool1)\n",
    "    \n",
    "\t# channel 2\n",
    "\tinput2 = Input(shape=(length,))\n",
    "\tembedding_layer_2 = embedding_layer(input2)\n",
    "\tconv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding_layer_2)\n",
    "\tdrop2 = Dropout(0.5)(conv2)\n",
    "\tpool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "\tflat2 = Flatten()(pool2)\n",
    "    \n",
    "\t# channel 3\n",
    "\tinput3 = Input(shape=(length,))\n",
    "\tembedding_layer_3 = embedding_layer(input3)\n",
    "\tconv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding_layer_3)\n",
    "\tdrop3 = Dropout(0.5)(conv3)\n",
    "\tpool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "\tflat3 = Flatten()(pool3)\n",
    "    \n",
    "\t# merge\n",
    "\tCNN_layer = concatenate([flat1, flat2, flat3])\n",
    "    \n",
    "\t# LSTM\n",
    "\tx = embedding_layer(input3)\n",
    "\tLSTM_layer = LSTM(128)(x)\n",
    "\n",
    "\tCNN_LSTM_layer = concatenate([LSTM_layer, CNN_layer])\n",
    "    \n",
    "\t# interpretation\n",
    "\tdense1 = Dense(10, activation='relu')(CNN_LSTM_layer)\n",
    "\toutputs = Dense(num_of_labels, activation='softmax')(dense1)\n",
    "\tmodel = Model(inputs=[input1, input2, input3], outputs=outputs)\n",
    "    \n",
    "\t# compile\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "\t# summarize\n",
    "\tprint(model.summary())\n",
    "    \n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 300, 300)     1025100     input_4[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 297, 32)      38432       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 295, 32)      57632       embedding_2[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 293, 32)      76832       embedding_2[2][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 297, 32)      0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 295, 32)      0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 293, 32)      0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 148, 32)      0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 147, 32)      0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 146, 32)      0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 4736)         0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 4704)         0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 4672)         0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 128)          219648      embedding_2[3][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 14112)        0           flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 14240)        0           lstm_2[0][0]                     \n",
      "                                                                 concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 10)           142410      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 6)            66          dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,560,120\n",
      "Trainable params: 535,020\n",
      "Non-trainable params: 1,025,100\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model= define_model(EMBEDDING_DIM,vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "def toCategorical(y):\n",
    "    y = to_categorical(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = toCategorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 701 samples, validate on 176 samples\n",
      "Epoch 1/20\n",
      "701/701 [==============================] - 12s 17ms/step - loss: 1.6760 - acc: 0.2539 - val_loss: 3.3705 - val_acc: 0.0398\n",
      "Epoch 2/20\n",
      "701/701 [==============================] - 9s 14ms/step - loss: 1.3261 - acc: 0.4579 - val_loss: 5.2184 - val_acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "701/701 [==============================] - 9s 13ms/step - loss: 1.1300 - acc: 0.5449 - val_loss: 6.8787 - val_acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "701/701 [==============================] - 12s 16ms/step - loss: 1.0058 - acc: 0.5877 - val_loss: 8.1012 - val_acc: 0.0000e+00\n",
      "Epoch 5/20\n",
      "701/701 [==============================] - 11s 16ms/step - loss: 0.8959 - acc: 0.6391 - val_loss: 9.2797 - val_acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "701/701 [==============================] - 22s 32ms/step - loss: 0.8244 - acc: 0.6534 - val_loss: 9.5983 - val_acc: 0.0000e+00\n",
      "Epoch 7/20\n",
      "701/701 [==============================] - 24s 34ms/step - loss: 0.7083 - acc: 0.7361 - val_loss: 9.9701 - val_acc: 0.0057\n",
      "Epoch 8/20\n",
      "701/701 [==============================] - 28s 40ms/step - loss: 0.6185 - acc: 0.7575 - val_loss: 10.3135 - val_acc: 0.0114\n",
      "Epoch 9/20\n",
      "701/701 [==============================] - 28s 40ms/step - loss: 0.5373 - acc: 0.8160 - val_loss: 10.8100 - val_acc: 0.0057\n",
      "Epoch 10/20\n",
      "701/701 [==============================] - 20s 29ms/step - loss: 0.4488 - acc: 0.8374 - val_loss: 11.3847 - val_acc: 0.0000e+00\n",
      "Epoch 11/20\n",
      "701/701 [==============================] - 23s 33ms/step - loss: 0.3866 - acc: 0.8688 - val_loss: 11.7607 - val_acc: 0.0000e+00\n",
      "Epoch 12/20\n",
      "701/701 [==============================] - 26s 38ms/step - loss: 0.3355 - acc: 0.8902 - val_loss: 12.0179 - val_acc: 0.0000e+00\n",
      "Epoch 13/20\n",
      "701/701 [==============================] - 23s 32ms/step - loss: 0.2882 - acc: 0.9301 - val_loss: 12.1809 - val_acc: 0.0057\n",
      "Epoch 14/20\n",
      "701/701 [==============================] - 21s 29ms/step - loss: 0.2529 - acc: 0.9358 - val_loss: 12.3040 - val_acc: 0.0000e+00\n",
      "Epoch 15/20\n",
      "701/701 [==============================] - 19s 27ms/step - loss: 0.2402 - acc: 0.9201 - val_loss: 12.4246 - val_acc: 0.0057\n",
      "Epoch 16/20\n",
      "701/701 [==============================] - 18s 25ms/step - loss: 0.2199 - acc: 0.9458 - val_loss: 12.3804 - val_acc: 0.0114\n",
      "Epoch 17/20\n",
      "701/701 [==============================] - 21s 30ms/step - loss: 0.1884 - acc: 0.9458 - val_loss: 12.5150 - val_acc: 0.0000e+00\n",
      "Epoch 18/20\n",
      "701/701 [==============================] - 20s 29ms/step - loss: 0.1621 - acc: 0.9643 - val_loss: 12.4372 - val_acc: 0.0227\n",
      "Epoch 19/20\n",
      "701/701 [==============================] - 20s 28ms/step - loss: 0.1485 - acc: 0.9686 - val_loss: 12.6193 - val_acc: 0.0227\n",
      "Epoch 20/20\n",
      "701/701 [==============================] - 22s 32ms/step - loss: 0.1394 - acc: 0.9672 - val_loss: 12.6117 - val_acc: 0.0057\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1480ceb9630>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X_train,X_train,X_train], y_train, epochs=20, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save('MConv_LSTM_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = load_data_and_labels('Hate_Speech_Test.csv',stop_word_list)\n",
    "y_test = toCategorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 300\n",
      "Vocabulary size: 1616\n",
      "\n",
      "[[222, 409, 410, 411, 412, 22], [145, 47, 81, 48, 146, 413, 414, 82, 109], [415, 223, 42, 224, 5, 416], [417, 418, 419, 225, 23], [110, 10, 420, 226, 227, 421, 228, 422, 423, 424, 229, 230, 147, 231, 1, 36, 425, 37, 426], [148, 427, 38, 428, 232, 83, 111, 38, 429, 1], [26, 233, 430, 149], [431, 432, 14, 433, 434], [435, 23, 234, 235, 236, 436], [26, 17, 47]]\n",
      "(323, 300)\n",
      "Test accuracy: 30.650155\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model \n",
    "    \n",
    "tokenizer_test = create_tokenizer(X_test)\n",
    "\n",
    "trainLength=300\n",
    "vocab_size=len(tokenizer_test.word_index)+1\n",
    "print('Max document length: %d' % trainLength)\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "print()\n",
    "X_test = encode_text(tokenizer_test, X_test, trainLength)\n",
    "print(X_test.shape)\n",
    "#trainX[2]\n",
    "\n",
    "# load the model\n",
    "model = load_model('MConv_LSTM_model.h5') \n",
    "\n",
    "# evaluate model on test dataset dataset\n",
    "loss, acc = model.evaluate([X_test,X_test,X_test], y_test, verbose=0)\n",
    "print('Test accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
