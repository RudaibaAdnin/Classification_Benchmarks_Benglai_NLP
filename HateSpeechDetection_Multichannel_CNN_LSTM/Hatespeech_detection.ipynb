{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    " \n",
    "# The GPU id to use, usually either \"0\" or \"1\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#importing libraries \n",
    "import sklearn\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline \n",
    "\n",
    "import string\n",
    "from os import listdir\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from pickle import dump\n",
    "from string import punctuation\n",
    "\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from pickle import load\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import csv, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Gender abusive hate': 0, 'Geopoitical Hate': 1, 'Religious Hate': 2, 'Political Normal': 3, 'Political Hate': 4, 'Personal Hate': 5}\n"
     ]
    }
   ],
   "source": [
    "categories = ['Gender abusive hate', 'Geopoitical Hate','Religious Hate','Political Normal','Political Hate','Personal Hate']\n",
    "num_of_labels=len(categories)\n",
    "categories_dict={key:value for value,key in enumerate(categories)}\n",
    "categories_inverse_dict={key:value for key,value in enumerate(categories)}\n",
    "print(categories_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['অতএব', 'অথচ']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "stop_words='../stopwords-bn.txt'\n",
    "text_data=[]\n",
    "with open(stop_words,'r',encoding='utf-8') as temp_output_file:\n",
    "    reader=csv.reader(temp_output_file, delimiter='\\n')\n",
    "    for row in reader:\n",
    "        text_data.append(row)\n",
    "stop_word_list=[x[0] for x in text_data]\n",
    "print(stop_word_list[0:2])\n",
    "print(type(stop_word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(dataset, filename):\n",
    "    dump(dataset, open(filename,'wb'))\n",
    "    print('Saved :%s' % filename)\n",
    "\n",
    "def load_doc(filename):\n",
    "    file=open(filename,'r',encoding='utf-8')\n",
    "    text=file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_doc(doc, stop_word_list):\n",
    "    # split into tokens by white space\n",
    "    sentences=list()\n",
    "    \n",
    "    for sentence in doc:\n",
    "        # remove punctuation from each token\n",
    "        \n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        tokens = [w.translate(table) for w in sentence.split(' ')]  # I belive word in sentence\n",
    "        sentences.append(' '.join(tokens))\n",
    "    return sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_labels(filename,stop_word_list):\n",
    "    doc=load_doc(filename)\n",
    "    x_doc=list()\n",
    "    label=list()\n",
    "    for line in doc.split('\\n'):\n",
    "        x_and_label=line.split('\\t')\n",
    "        #print(x_and_label)\n",
    "        x_doc.append(x_and_label[1])\n",
    "        label.append(categories_dict[x_and_label[0]])\n",
    "        \n",
    "    #print(x_doc)\n",
    "    #print(label)\n",
    "    trainX=clean_doc(x_doc, stopwords)\n",
    "    return trainX, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX,trainY=load_data_and_labels('../Hate_Speech_Complete.csv',stop_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['এদের কারনে এলিয়েনরা পৃথিবীতে বেড়াতে আসে না।', 'এই গান গুলা হুইন্না আমি এখন আমি পাবনা তে আসি', 'ভাই ছোট আজাদ চুদনাপানা না করলেই কি না ', 'সব মাদার চোদ। সালারা তোদের কাছ থেকে কি শিখব ', 'বাইরে থাক আর গুঁতাগুঁতি কর ', 'ইংলিশ এ Wiz khalifa Weed টানে আর বাঙালিরারা টানলেই দোষ ধইরা ফেল্লা', 'আসোলে তুমি নিজেই বলসিলা বাংগালি না আমরা। জাতটাই ত খারাপ', 'আর আমাদের এই খাইস্টা জাতের মধ্যে তুমিও পড় সো রক্ত ত মিশশা আসেই জিনিস টা', 'শালা তুই সালমান রিদিয়া আসিফসব পাকনাপাকনা পুলাপাইন ', 'এসব আবাল গুলা আমাদের সুন্দর বিনোদন থেকে দূরে রাখে। ']\n"
     ]
    }
   ],
   "source": [
    "print(trainX[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "    tokenizer=Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "def encode_text(tokenizer, lines, length):\n",
    "    #print(lines)\n",
    "    encoded=tokenizer.texts_to_sequences(lines)\n",
    "    print(encoded[0:10])\n",
    "    padded= pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=create_tokenizer(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 300\n",
      "Vocabulary size: 5034\n",
      "\n",
      "[[194, 339, 1537, 636, 1538, 637, 70], [2, 340, 195, 1539, 33, 52, 33, 1540, 341, 908], [21, 221, 253, 1541, 1, 1542, 6, 1], [12, 111, 1543, 487, 34, 1544, 28, 6, 1545], [408, 909, 5, 1546, 179], [254, 66, 1547, 1548, 1549, 1550, 5, 1551, 1552, 910, 911, 1553], [1554, 222, 912, 1555, 1556, 1, 1557, 1558, 255, 112], [5, 23, 2, 1559, 1560, 151, 1561, 1562, 488, 489, 255, 1563, 1564, 1565, 53], [39, 7, 913, 1566, 1567, 1568, 914], [180, 162, 195, 23, 256, 1569, 28, 1570, 1571]]\n",
      "(1400, 300)\n"
     ]
    }
   ],
   "source": [
    "trainLength=300\n",
    "vocab_size=len(tokenizer.word_index)+1\n",
    "print('Max document length: %d' % trainLength)\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "print()\n",
    "trainX = encode_text(tokenizer, trainX, trainLength)\n",
    "print(trainX.shape)\n",
    "#trainX[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bharaj/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "NUM_WORDS=20000\n",
    "word_vectors = Word2Vec.load('../posts.bin')\n",
    "EMBEDDING_DIM=300\n",
    "vocabulary_size=len(tokenizer.word_index)+1\n",
    "word_index=tokenizer.word_index\n",
    "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i>=NUM_WORDS:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector=word_vectors[word]\n",
    "        embedding_matrix[i]=embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
    "\n",
    "del(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer=Embedding(vocabulary_size, EMBEDDING_DIM, weights=[embedding_matrix],trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model(length, vocab_size):\n",
    "\t# channel 1\n",
    "\tinput1 = Input(shape=(length,))\n",
    "\tembedding_layer_1 = embedding_layer(input1)\n",
    "\tconv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding_layer_1)\n",
    "\tdrop1 = Dropout(0.5)(conv1)\n",
    "\tpool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "\tflat1 = Flatten()(pool1)\n",
    "    \n",
    "\t# channel 2\n",
    "\tinput2 = Input(shape=(length,))\n",
    "\tembedding_layer_2 = embedding_layer(input2)\n",
    "\tconv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding_layer_2)\n",
    "\tdrop2 = Dropout(0.5)(conv2)\n",
    "\tpool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "\tflat2 = Flatten()(pool2)\n",
    "    \n",
    "\t# channel 3\n",
    "\tinput3 = Input(shape=(length,))\n",
    "\tembedding_layer_3 = embedding_layer(input3)\n",
    "\tconv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding_layer_3)\n",
    "\tdrop3 = Dropout(0.5)(conv3)\n",
    "\tpool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "\tflat3 = Flatten()(pool3)\n",
    "    \n",
    "\t# merge\n",
    "\tCNN_layer = concatenate([flat1, flat2, flat3])\n",
    "    \n",
    "\t# LSTM\n",
    "\tx = embedding_layer(input3)\n",
    "\tLSTM_layer = LSTM(128)(x)\n",
    "\n",
    "\tCNN_LSTM_layer = concatenate([LSTM_layer, CNN_layer])\n",
    "    \n",
    "\t# interpretation\n",
    "\tdense1 = Dense(10, activation='relu')(CNN_LSTM_layer)\n",
    "\toutputs = Dense(num_of_labels, activation='softmax')(dense1)\n",
    "\tmodel = Model(inputs=[input1, input2, input3], outputs=outputs)\n",
    "    \n",
    "\t# compile\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "\t# summarize\n",
    "\tprint(model.summary())\n",
    "    \n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 300, 300)     1510200     input_10[0][0]                   \n",
      "                                                                 input_11[0][0]                   \n",
      "                                                                 input_12[0][0]                   \n",
      "                                                                 input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 297, 32)      38432       embedding_1[12][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 295, 32)      57632       embedding_1[13][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 293, 32)      76832       embedding_1[14][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 297, 32)      0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 295, 32)      0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 293, 32)      0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 148, 32)      0           dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 147, 32)      0           dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 146, 32)      0           dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 4736)         0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 4704)         0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 4672)         0           max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 128)          219648      embedding_1[15][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 14112)        0           flatten_10[0][0]                 \n",
      "                                                                 flatten_11[0][0]                 \n",
      "                                                                 flatten_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 14240)        0           lstm_4[0][0]                     \n",
      "                                                                 concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 10)           142410      concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 6)            66          dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,045,220\n",
      "Trainable params: 2,045,220\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model= define_model(300,5034)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_binary = to_categorical(trainY)\n",
    "print(y_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1120 samples, validate on 280 samples\n",
      "Epoch 1/1\n",
      "1120/1120 [==============================] - 9s 8ms/step - loss: 1.7872 - acc: 0.3571 - val_loss: 1.8007 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7333dbb470>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([trainX,trainX,trainX], array(y_binary), epochs=1, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
