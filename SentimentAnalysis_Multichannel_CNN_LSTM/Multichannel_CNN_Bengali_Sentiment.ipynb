{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline \n",
    "\n",
    "import string\n",
    "from os import listdir\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from pickle import dump\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['positive', 'negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a dataset to file\n",
    "def save_dataset(dataset, filename):\n",
    "\tdump(dataset, open(filename, 'wb'))\n",
    "\tprint('Saved: %s' % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r', encoding='utf-8')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "stop_words = 'stopwords_bn.txt'\n",
    "text_data = []\n",
    "\n",
    "with open(stop_words, 'r', encoding='utf-8') as temp_output_file:\n",
    "    reader = csv.reader(temp_output_file, delimiter='\\n')\n",
    "    for row in reader:\n",
    "        text_data.append(row)\n",
    "\n",
    "stop_word_list = [x[0] for x in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "    \n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "    \n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "    \n",
    "\t# filter out stop words\n",
    "\tstop_words = stop_word_list\n",
    "\ttokens = [w for w in tokens if not w in stop_words]\n",
    "    \n",
    "\t# filter out short tokens\n",
    "\ttokens = [word for word in tokens if len(word) > 1]\n",
    "\ttokens = ' '.join(tokens)\n",
    "\treturn tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_labels(positive_data_file, negative_data_file):\n",
    "    \n",
    "    # load the positive doc\n",
    "    pos_doc = load_doc(positive_data_file)\n",
    "    \n",
    "    # clean doc\n",
    "    positive_examples = clean_doc(pos_doc)\n",
    "    \n",
    "    # load the negative doc\n",
    "    neg_doc = load_doc(negative_data_file)\n",
    "    negative_examples = clean_doc(neg_doc)\n",
    "    \n",
    "    # Split by words\n",
    "    x_text = positive_examples + negative_examples\n",
    "    \n",
    "    # Generate labels\n",
    "    positive_labels = [[0] for _ in positive_examples]\n",
    "    negative_labels = [[1] for _ in negative_examples]\n",
    "    trainy = [0 for _ in positive_examples] + [1 for _ in negative_examples]\n",
    "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    \n",
    "    return [x_text, trainy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "\n",
    "#Training dataset\n",
    "positive_data_file = 'C:/Users/admin-karim/Downloads/tmp/train/bangla.pos'\n",
    "negative_data_file = 'C:/Users/admin-karim/Downloads/tmp/train/bangla.neg'\n",
    "\n",
    "trainX, trainY = load_data_and_labels(positive_data_file, negative_data_file)\n",
    "\n",
    "#Testing dataset\n",
    "positive_data_file = 'C:/Users/admin-karim/Downloads/tmp/test/bangla.pos'\n",
    "negative_data_file = 'C:/Users/admin-karim/Downloads/tmp/test/bangla.neg'\n",
    "\n",
    "testX, testY = load_data_and_labels(positive_data_file, negative_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: train.pkl\n",
      "Saved: test.pkl\n"
     ]
    }
   ],
   "source": [
    "save_dataset([trainX, trainY], 'train.pkl')\n",
    "save_dataset([testX, testY], 'test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from pickle import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a clean dataset\n",
    "def load_dataset(filename):\n",
    "\treturn load(open(filename, 'rb'))\n",
    " \n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer\n",
    " \n",
    "# calculate the maximum document length\n",
    "def max_length(lines):\n",
    "\treturn len(lines)\n",
    " \n",
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "\t# integer encode\n",
    "\tencoded = tokenizer.texts_to_sequences(lines)\n",
    "\t# pad encoded sequences\n",
    "\tpadded = pad_sequences(encoded, maxlen = length, padding = 'post')\n",
    "\treturn padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\admin-karim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "NUM_WORDS=20000\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "word_vectors = Word2Vec.load('C:/Users/admin-karim/Desktop/BengWord2Vec/posts.bin')\n",
    "\n",
    "EMBEDDING_DIM=300\n",
    "vocabulary_size = len(tokenizer.word_index) + 1\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i>=NUM_WORDS:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
    "\n",
    "del(word_vectors)\n",
    "\n",
    "from keras.layers import Embedding\n",
    "embedding_layer = Embedding(vocabulary_size, EMBEDDING_DIM, weights=[embedding_matrix], trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model(length, vocab_size):\n",
    "\t# channel 1\n",
    "\tinput1 = Input(shape=(length,))\n",
    "\tembedding_layer_1 = embedding_layer(input1)\n",
    "\tconv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding_layer_1)\n",
    "\tdrop1 = Dropout(0.5)(conv1)\n",
    "\tpool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "\tflat1 = Flatten()(pool1)\n",
    "    \n",
    "\t# channel 2\n",
    "\tinput2 = Input(shape=(length,))\n",
    "\tembedding_layer_2 = embedding_layer(input2)\n",
    "\tconv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding_layer_2)\n",
    "\tdrop2 = Dropout(0.5)(conv2)\n",
    "\tpool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "\tflat2 = Flatten()(pool2)\n",
    "    \n",
    "\t# channel 3\n",
    "\tinput3 = Input(shape=(length,))\n",
    "\tembedding_layer_3 = embedding_layer(input3)\n",
    "\tconv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding_layer_3)\n",
    "\tdrop3 = Dropout(0.5)(conv3)\n",
    "\tpool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "\tflat3 = Flatten()(pool3)\n",
    "    \n",
    "\t# merge\n",
    "\tCNN_layer = concatenate([flat1, flat2, flat3])\n",
    "    \n",
    "\t# LSTM\n",
    "\tx = embedding_layer(input3)\n",
    "\tLSTM_layer = LSTM(128)(x)\n",
    "\n",
    "\tCNN_LSTM_layer = concatenate([LSTM_layer, CNN_layer])\n",
    "    \n",
    "\t# interpretation\n",
    "\tdense1 = Dense(10, activation='relu')(CNN_LSTM_layer)\n",
    "\toutputs = Dense(1, activation='sigmoid')(dense1)\n",
    "\tmodel = Model(inputs=[input1, input2, input3], outputs=outputs)\n",
    "    \n",
    "\t# compile\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "\t# summarize\n",
    "\tprint(model.summary())\n",
    "    \n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 300\n",
      "Vocabulary size: 73\n",
      "(12486, 300)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_29 (InputLayer)           (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_27 (InputLayer)           (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_28 (InputLayer)           (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_25 (Embedding)        (None, 300, 300)     21900       input_27[0][0]                   \n",
      "                                                                 input_28[0][0]                   \n",
      "                                                                 input_29[0][0]                   \n",
      "                                                                 input_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 297, 32)      38432       embedding_25[6][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 295, 32)      57632       embedding_25[7][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 293, 32)      76832       embedding_25[8][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 297, 32)      0           conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 295, 32)      0           conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 293, 32)      0           conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 148, 32)      0           dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 147, 32)      0           dropout_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 146, 32)      0           dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_25 (Flatten)            (None, 4736)         0           max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_26 (Flatten)            (None, 4704)         0           max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_27 (Flatten)            (None, 4672)         0           max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                   (None, 128)          219648      embedding_25[9][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 14112)        0           flatten_25[0][0]                 \n",
      "                                                                 flatten_26[0][0]                 \n",
      "                                                                 flatten_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 14240)        0           lstm_9[0][0]                     \n",
      "                                                                 concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 10)           142410      concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 1)            11          dense_15[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 556,865\n",
      "Trainable params: 556,865\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/1\n",
      "12486/12486 [==============================] - 373s 30ms/step - loss: 0.6847 - acc: 0.5585\n"
     ]
    }
   ],
   "source": [
    "# load training dataset\n",
    "trainLines, trainLabels = load_dataset('train.pkl')\n",
    "\n",
    "# create tokenizer\n",
    "tokenizer = create_tokenizer(trainLines)\n",
    "\n",
    "# calculate max document length\n",
    "trainLength = 300\n",
    "\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % trainLength)\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "\n",
    "# encode data\n",
    "trainX = encode_text(tokenizer, trainLines, trainLength)\n",
    "print(trainX.shape)\n",
    " \n",
    "# define model\n",
    "model = define_model(trainLength, vocab_size)\n",
    "\n",
    "# fit model\n",
    "model.fit([trainX,trainX,trainX], array(trainLabels), epochs=1, batch_size=128)\n",
    "\n",
    "# save the model\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
    "\n",
    "plot_model(model, show_shapes=True, to_file='multichannel.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 300\n",
      "Vocabulary size: 62\n",
      "(2703, 300)\n",
      "Test accuracy: 50.351461\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    " \n",
    "testLines, testLabels = load_dataset('test.pkl')\n",
    " \n",
    "# create tokenizer\n",
    "tokenizer = create_tokenizer(testLines)\n",
    "\n",
    "# calculate max document length\n",
    "length = trainLength\n",
    "\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % length)\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "\n",
    "# encode data\n",
    "testX = encode_text(tokenizer, testLines, length)\n",
    "print(testX.shape)\n",
    "\n",
    "# load the model\n",
    "model = load_model('model.h5') \n",
    "\n",
    "# evaluate model on test dataset dataset\n",
    "loss, acc = model.evaluate([testX,testX,testX],array(testLabels), verbose=0)\n",
    "print('Test accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
