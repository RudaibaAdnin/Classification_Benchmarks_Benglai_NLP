{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline \n",
    "\n",
    "import string\n",
    "from os import listdir\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from pickle import dump\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['positive', 'negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a dataset to file\n",
    "def save_dataset(dataset, filename):\n",
    "\tdump(dataset, open(filename, 'wb'))\n",
    "\tprint('Saved: %s' % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r', encoding='utf-8')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "stop_words = 'stopwords_bn.txt'\n",
    "text_data = []\n",
    "\n",
    "with open(stop_words, 'r', encoding='utf-8') as temp_output_file:\n",
    "    reader = csv.reader(temp_output_file, delimiter='\\n')\n",
    "    for row in reader:\n",
    "        text_data.append(row)\n",
    "\n",
    "stop_word_list = [x[0] for x in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "    \n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "    \n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "    \n",
    "\t# filter out stop words\n",
    "\tstop_words = stop_word_list\n",
    "\ttokens = [w for w in tokens if not w in stop_words]\n",
    "    \n",
    "\t# filter out short tokens\n",
    "\ttokens = [word for word in tokens if len(word) > 1]\n",
    "\ttokens = ' '.join(tokens)\n",
    "\treturn tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_labels(positive_data_file, negative_data_file):\n",
    "    \n",
    "    # load the positive doc\n",
    "    pos_doc = load_doc(positive_data_file)\n",
    "    \n",
    "    # clean doc\n",
    "    positive_examples = clean_doc(pos_doc)\n",
    "    \n",
    "    # load the negative doc\n",
    "    neg_doc = load_doc(negative_data_file)\n",
    "    negative_examples = clean_doc(neg_doc)\n",
    "    \n",
    "    # Split by words\n",
    "    x_text = positive_examples + negative_examples\n",
    "    \n",
    "    # Generate labels\n",
    "    positive_labels = [[0] for _ in positive_examples]\n",
    "    negative_labels = [[1] for _ in negative_examples]\n",
    "    trainy = [0 for _ in positive_examples] + [1 for _ in negative_examples]\n",
    "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    \n",
    "    return [x_text, trainy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "\n",
    "#Training dataset\n",
    "positive_data_file = 'C:/Users/admin-karim/Downloads/tmp/train/bangla.pos'\n",
    "negative_data_file = 'C:/Users/admin-karim/Downloads/tmp/train/bangla.neg'\n",
    "\n",
    "trainX, trainY = load_data_and_labels(positive_data_file, negative_data_file)\n",
    "\n",
    "#Testing dataset\n",
    "positive_data_file = 'C:/Users/admin-karim/Downloads/tmp/test/bangla.pos'\n",
    "negative_data_file = 'C:/Users/admin-karim/Downloads/tmp/test/bangla.neg'\n",
    "\n",
    "testX, testY = load_data_and_labels(positive_data_file, negative_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: train.pkl\n",
      "Saved: test.pkl\n"
     ]
    }
   ],
   "source": [
    "save_dataset([trainX, trainY], 'train.pkl')\n",
    "save_dataset([testX, testY], 'test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from pickle import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a clean dataset\n",
    "def load_dataset(filename):\n",
    "\treturn load(open(filename, 'rb'))\n",
    " \n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer\n",
    " \n",
    "# calculate the maximum document length\n",
    "def max_length(lines):\n",
    "\treturn len(lines)\n",
    " \n",
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "\t# integer encode\n",
    "\tencoded = tokenizer.texts_to_sequences(lines)\n",
    "\t# pad encoded sequences\n",
    "\tpadded = pad_sequences(encoded, maxlen = length, padding = 'post')\n",
    "\treturn padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model(length, vocab_size):\n",
    "\t# channel 1\n",
    "\tinput1 = Input(shape=(length,))\n",
    "\tembedding1 = Embedding(vocab_size, 18)(input1)\n",
    "\tconv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
    "\tdrop1 = Dropout(0.5)(conv1)\n",
    "\tpool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "\tflat1 = Flatten()(pool1)\n",
    "    \n",
    "\t# channel 2\n",
    "\tinput2 = Input(shape=(length,))\n",
    "\tembedding2 = Embedding(vocab_size, 15)(input2)\n",
    "\tconv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
    "\tdrop2 = Dropout(0.5)(conv2)\n",
    "\tpool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "\tflat2 = Flatten()(pool2)\n",
    "    \n",
    "\t# channel 3\n",
    "\tinput3 = Input(shape=(length,))\n",
    "\tembedding3 = Embedding(vocab_size, 12)(input3)\n",
    "\tconv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
    "\tdrop3 = Dropout(0.5)(conv3)\n",
    "\tpool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "\tflat3 = Flatten()(pool3)\n",
    "    \n",
    "\t# merge\n",
    "\tCNN_layer = concatenate([flat1, flat2, flat3])\n",
    "    \n",
    "\t# LSTM\n",
    "\tx = Embedding(vocab_size, 20)(input1)\n",
    "\tLSTM_layer = LSTM(128)(x)\n",
    "\n",
    "\tCNN_LSTM_layer = concatenate([LSTM_layer, CNN_layer])\n",
    "    \n",
    "\t# interpretation\n",
    "\tdense1 = Dense(10, activation='relu')(CNN_LSTM_layer)\n",
    "\toutputs = Dense(1, activation='sigmoid')(dense1)\n",
    "\tmodel = Model(inputs=[input1, input2, input3], outputs=outputs)\n",
    "    \n",
    "\t# compile\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "\t# summarize\n",
    "\tprint(model.summary())\n",
    "\t#plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
    "    \n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 12486\n",
      "Vocabulary size: 73\n",
      "(12486, 12486)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 12486)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 12486)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 12486)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 12486, 18)    1314        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 12486, 15)    1095        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 12486, 12)    876         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 12483, 32)    2336        embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 12481, 32)    2912        embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 12479, 32)    3104        embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 12483, 32)    0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 12481, 32)    0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 12479, 32)    0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 6241, 32)     0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 6240, 32)     0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 6239, 32)     0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 12486, 20)    1460        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 199712)       0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 199680)       0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 199648)       0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 128)          76288       embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 599040)       0           flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 599168)       0           lstm_1[0][0]                     \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 10)           5991690     concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            11          dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 6,081,086\n",
      "Trainable params: 6,081,086\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/1\n",
      "  512/12486 [>.............................] - ETA: 38:19:08 - loss: 6.8151 - acc: 0.4785"
     ]
    }
   ],
   "source": [
    "# load training dataset\n",
    "trainLines, trainLabels = load_dataset('train.pkl')\n",
    "\n",
    "# create tokenizer\n",
    "tokenizer = create_tokenizer(trainLines)\n",
    "\n",
    "# calculate max document length\n",
    "trainLength = max_length(trainLines)\n",
    "\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % trainLength)\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "\n",
    "# encode data\n",
    "trainX = encode_text(tokenizer, trainLines, trainLength)\n",
    "print(trainX.shape)\n",
    " \n",
    "# define model\n",
    "model = define_model(trainLength, vocab_size)\n",
    "\n",
    "# fit model\n",
    "model.fit([trainX,trainX,trainX], array(trainLabels), epochs=1, batch_size=128)\n",
    "\n",
    "# save the model\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 12486\n",
      "Vocabulary size: 62\n",
      "(2703, 12486)\n",
      "Test accuracy: 47.132815\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    " \n",
    "testLines, testLabels = load_dataset('test.pkl')\n",
    " \n",
    "# create tokenizer\n",
    "tokenizer = create_tokenizer(testLines)\n",
    "\n",
    "# calculate max document length\n",
    "length = trainLength\n",
    "\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % length)\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "\n",
    "# encode data\n",
    "testX = encode_text(tokenizer, testLines, length)\n",
    "print(testX.shape)\n",
    "\n",
    "# load the model\n",
    "model = load_model('model.h5') \n",
    "\n",
    "# evaluate model on test dataset dataset\n",
    "loss, acc = model.evaluate([testX,testX,testX],array(testLabels), verbose=0)\n",
    "print('Test accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
